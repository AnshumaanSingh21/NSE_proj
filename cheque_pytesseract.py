# -*- coding: utf-8 -*-
"""cheque_pytesseract.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pqCamcpHl_-mQi7PazQyS4dQgL6LUUXf
"""

# Install YOLOv8 and Tesseract
!pip install -q ultralytics opencv-python-headless
!apt install -y tesseract-ocr

# Install pytesseract and Tesseract OCR engine
!pip install pytesseract
!apt install -y tesseract-ocr

import cv2, os, csv
import pytesseract
from PIL import Image
from ultralytics import YOLO
from google.colab import drive

# === Mount Google Drive ===
drive.mount('/content/drive')

# === Configuration ===
image_dir        = "/content/drive/MyDrive/cheque_project/dataset"
model_path       = "/content/drive/MyDrive/cheque_project/best.pt"
output_csv_path  = "/content/drive/MyDrive/cheque_project/cheque_data1.csv"
output_txt_dir   = os.path.join(image_dir, "results")
conf_threshold   = 0.10
image_exts       = (".jpg", ".jpeg", ".png")
expected_classes = ['account number', 'bank name', 'ifsc number', 'person name']

# === Setup pytesseract path (already installed on Colab) ===
pytesseract.pytesseract.tesseract_cmd = "/usr/bin/tesseract"

# === OCR preprocessing function ===
def preprocess_for_ocr(cropped_img):
    gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)
    big = cv2.resize(gray, None, fx=2, fy=2, interpolation=cv2.INTER_LINEAR)
    denoise = cv2.fastNlMeansDenoising(big, h=30)
    _, thr = cv2.threshold(denoise, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    return Image.fromarray(thr)

# === Load model ===
model = YOLO(model_path)

# === Create output folder ===
os.makedirs(output_txt_dir, exist_ok=True)

# === Process images and write CSV ===
with open(output_csv_path, "w", newline="", encoding="utf-8") as csv_file:
    writer = csv.writer(csv_file)
    writer.writerow(["image"] + [c.upper() for c in expected_classes])

    for fname in sorted(os.listdir(image_dir)):
        if not fname.lower().endswith(image_exts):
            continue

        img_path = os.path.join(image_dir, fname)
        image = cv2.imread(img_path)
        if image is None:
            print(f"‚ö†Ô∏è Skipping unreadable file: {fname}")
            continue

        results = model.predict(image, conf=conf_threshold, verbose=False)[0]
        h_img, w_img = image.shape[:2]
        detected_text = {label: "Not Detected" for label in expected_classes}

        for box in results.boxes.data.tolist():
            x1, y1, x2, y2, score, class_id = box
            label = model.names[int(class_id)].lower()

            if label not in expected_classes:
                continue

            x1, y1 = max(0, int(x1)), max(0, int(y1))
            x2, y2 = min(w_img, int(x2)), min(h_img, int(y2))
            crop = image[y1:y2, x1:x2]
            pil_img = preprocess_for_ocr(crop)
            text = pytesseract.image_to_string(
                      pil_img,
                      lang='eng',
              config='--oem 3 --psm 7'
            ).strip().replace("\n", " ")



            if len(text) >= 3:
                detected_text[label] = text

            print(f"{fname} | {label:<15} conf {score:.2f} | OCR: {text}")

        # Write per-image TXT
        txt_path = os.path.join(output_txt_dir, os.path.splitext(fname)[0] + ".txt")
        with open(txt_path, "w", encoding="utf-8") as f_txt:
            for label in expected_classes:
                f_txt.write(f"{label.upper()}: {detected_text[label]}\n")

        # Write to CSV
        writer.writerow([fname] + [detected_text[c] for c in expected_classes])

print(f"\n‚úÖ All done! CSV saved to ‚Üí {output_csv_path}")
print(f"üóÇÔ∏è Per-image results in ‚Üí {output_txt_dir}")

if label in ['ifsc number', 'person name']:
    debug_path = os.path.join(output_txt_dir, f"{os.path.splitext(fname)[0]}_{label}.png")
    pil_img.save(debug_path)

!pip install fuzzywuzzy

import os
import cv2
import csv
import pytesseract
import re
import numpy as np
from PIL import Image
from ultralytics import YOLO
from skimage import restoration
from skimage.metrics import structural_similarity

# ========== CONFIGURATION ==========
image_dir = "/content/drive/MyDrive/cheque_project/dataset"
output_csv_path = "/content/drive/MyDrive/cheque_project/cheque_data_enhanced.csv"
output_txt_dir = "/content/drive/MyDrive/cheque_project/enhanced_results_txts"
image_exts = (".jpg", ".jpeg", ".png")
expected_classes = ["bank name", "drawer name", "ifsc code", "account no"]

os.makedirs(output_txt_dir, exist_ok=True)

# ========== LOAD YOLOv8 MODEL ==========
model_path = "/content/drive/MyDrive/cheque_project/best.pt"
model = YOLO(model_path)

# ========== ADVANCED PREPROCESSING ==========
def enhance_image_quality(img, min_height=30):
    """Apply advanced preprocessing for OCR optimization"""
    if len(img.shape) == 3:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    else:
        gray = img

    # Handle small images with intelligent upscaling
    h, w = gray.shape
    if h < min_height:
        scale_factor = min(3.0, min_height / h)  # Limit scaling to 3x
        new_width = int(w * scale_factor)
        gray = cv2.resize(gray, (new_width, min_height), interpolation=cv2.INTER_CUBIC)
        h, w = gray.shape

    if h == 0 or w == 0:
        return gray

    # Deblurring using Wiener filter
    psf = np.ones((3, 3)) / 9  # Point spread function
    deblurred = restoration.wiener(gray, psf, 1.5)
    deblurred = np.uint8(deblurred * 255 / np.max(deblurred))

    # Advanced noise reduction
    denoised = cv2.fastNlMeansDenoising(
        deblurred,
        h=15,
        templateWindowSize=9,
        searchWindowSize=21
    )

    # Contrast enhancement with dynamic CLAHE
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(min(8, h//50), min(8, w//50)))
    contrast_enhanced = clahe.apply(denoised)

    # Edge-preserving sharpening
    gaussian = cv2.GaussianBlur(contrast_enhanced, (0, 0), 3.0)
    sharpened = cv2.addWeighted(contrast_enhanced, 2.5, gaussian, -1.5, 0)

    # Sauvola's adaptive binarization for better text extraction
    sauvola_thresh = cv2.ximgproc.niBlackThreshold(
        sharpened,
        maxValue=255,
        type=cv2.THRESH_BINARY,
        blockSize=31,
        k=0.1,
        binarizationMethod=cv2.ximgproc.BINARIZATION_SAUVOLA
    )

    return sauvola_thresh

# ========== ENHANCED REGEX HELPERS ==========
def extract_ifsc(text):
    patterns = [
        r'\b[A-Z]{4}0[A-Z0-9]{6}\b',
        r'\b[A-Z]{4}0\d{6}\b',
        r'\b[A-Z]{4}0\w{6}\b'
    ]
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(0)
    return "Not Detected"

def extract_account_no(text):
    # Account numbers can be 9-18 digits, sometimes with spaces
    clean_text = re.sub(r'\s', '', text)  # Remove spaces
    match = re.search(r'\d{9,18}', clean_text)
    return match.group(0) if match else "Not Detected"

def extract_bank_name(text):
    lines = text.replace(':', ' ').replace('-', ' ').split('\n')
    for line in lines:
        clean = re.sub(r'[^A-Za-z\s]', '', line).strip()
        if "BANK" in clean.upper() and len(clean.split()) >= 2:
            return clean
    return "Not Detected"

def extract_name(text):
    lines = text.split('\n')
    exclusion_terms = ["ACCOUNT", "SAVINGS", "A/C", "BANK", "IFSC", "CODE", "CHEQUE", "BRANCH"]

    for line in lines:
        clean = re.sub(r'[^A-Za-z\s]', '', line).strip()
        if (clean.isupper() and 2 <= len(clean.split()) <= 4 and
            not any(term in clean.upper() for term in exclusion_terms)):
            return clean
    return "Not Detected"

# ========== MULTI-STAGE OCR PROCESSING ==========
def run_enhanced_ocr(image, label, is_full_image=False):
    """Run OCR with multiple strategies and validations"""
    best_text = ""
    best_confidence = 0

    # Configuration strategies
    strategies = [
        {'scale': 1.0, 'config': '--oem 3 --psm 6'},    # Uniform block
        {'scale': 1.5, 'config': '--oem 3 --psm 7'},     # Line of text
        {'scale': 2.0, 'config': '--oem 3 --psm 11'},    # Sparse text
        {'scale': 1.0, 'config': '--oem 3 --psm 12'},    # Sparse text with OSD
    ]

    if is_full_image:
        strategies = [
            {'scale': 1.0, 'config': '--oem 3 --psm 6'},
            {'scale': 1.0, 'config': '--oem 3 --psm 11'},
        ]

    for strategy in strategies:
        scale = strategy['scale']
        config = strategy['config']

        # Scale image while preserving aspect ratio
        if scale != 1.0:
            h, w = image.shape[:2]
            scaled_img = cv2.resize(image, (int(w * scale), int(h * scale)),
                                   interpolation=cv2.INTER_CUBIC)
        else:
            scaled_img = image

        # Convert to PIL format
        pil_img = Image.fromarray(scaled_img)

        # Run OCR and get confidence data
        ocr_data = pytesseract.image_to_data(
            pil_img,
            lang='eng',
            config=config,
            output_type=pytesseract.Output.DICT
        )

        # Calculate average confidence of non-empty words
        confidences = [float(c) for i, c in enumerate(ocr_data['conf'])
                      if int(c) > 0 and ocr_data['text'][i].strip()]
        avg_confidence = np.mean(confidences) if confidences else 0

        # Extract combined text
        text = ' '.join([t for t in ocr_data['text'] if t.strip()])

        # Validate based on content type
        if label == "ifsc code" and extract_ifsc(text) != "Not Detected":
            return text, 100  # Return immediately for valid pattern

        if label == "account no" and extract_account_no(text) != "Not Detected":
            return text, 100

        # Update best result based on confidence and content
        if avg_confidence > best_confidence or (avg_confidence == best_confidence and len(text) > len(best_text)):
            best_text = text
            best_confidence = avg_confidence

    return best_text, best_confidence

# ========== MAIN PROCESSING ==========
with open(output_csv_path, "w", newline="", encoding="utf-8") as csv_file:
    writer = csv.writer(csv_file)
    writer.writerow(["image"] + [c.upper() for c in expected_classes])

    for fname in sorted(os.listdir(image_dir)):
        if not fname.lower().endswith(image_exts):
            continue

        img_path = os.path.join(image_dir, fname)
        image = cv2.imread(img_path)
        if image is None:
            print(f"‚ö†Ô∏è Skipping unreadable file: {fname}")
            continue

        detected_text = {label: "Not Detected" for label in expected_classes}
        h_img, w_img = image.shape[:2]

        # Run YOLOv8 detection
        results = model.predict(image, conf=0.2, verbose=False)[0]  # Lower confidence for small boxes

        # Sort boxes by area (smallest first) to prioritize small regions
        sorted_boxes = sorted(results.boxes.data.tolist(),
                             key=lambda box: (box[2]-box[0])*(box[3]-box[1]))

        for box in sorted_boxes:
            x1, y1, x2, y2, score, class_id = box
            label = model.names[int(class_id)].lower()

            if label not in expected_classes:
                continue

            # Adaptive padding based on box size
            box_area = (x2 - x1) * (y2 - y1)
            pad = max(2, min(15, int(box_area**0.25)))  # Dynamic padding

            x1, y1 = max(0, int(x1)-pad), max(0, int(y1)-pad)
            x2, y2 = min(w_img, int(x2)+pad), min(h_img, int(y2)+pad)

            if x2 <= x1 or y2 <= y1:
                continue

            crop = image[y1:y2, x1:x2]

            # Skip very small crops
            if crop.size < 100:  # Less than 10x10 pixels
                continue

            crop_proc = enhance_image_quality(crop)

            # Run enhanced OCR with multiple strategies
            text, confidence = run_enhanced_ocr(crop_proc, label)

            # Apply extraction based on field type
            if label == "ifsc code":
                result = extract_ifsc(text)
            elif label == "drawer name":
                result = extract_name(text)
            elif label == "account no":
                result = extract_account_no(text)
            elif label == "bank name":
                result = extract_bank_name(text)

            # Only update if we found a better result
            if result != "Not Detected" or detected_text[label] == "Not Detected":
                detected_text[label] = result

            print(f"{fname} | {label:<15} ({score:.2f}, conf:{confidence:.1f}) ‚Üí {detected_text[label]}")

        # Fallback: full image OCR with enhanced preprocessing
        if any(v == "Not Detected" for v in detected_text.values()):
            full_proc = enhance_image_quality(image)
            full_text, _ = run_enhanced_ocr(full_proc, "", is_full_image=True)

            if detected_text["ifsc code"] == "Not Detected":
                detected_text["ifsc code"] = extract_ifsc(full_text)
            if detected_text["drawer name"] == "Not Detected":
                detected_text["drawer name"] = extract_name(full_text)
            if detected_text["account no"] == "Not Detected":
                detected_text["account no"] = extract_account_no(full_text)
            if detected_text["bank name"] == "Not Detected":
                detected_text["bank name"] = extract_bank_name(full_text)

        # Write to text file
        txt_path = os.path.join(output_txt_dir, os.path.splitext(fname)[0] + ".txt")
        with open(txt_path, "w", encoding="utf-8") as f_txt:
            f_txt.write(f"FILE: {fname}\n")
            for label in expected_classes:
                label_upper = label.upper()
                f_txt.write(f"{label_upper}: {detected_text[label]}\n")

        # Write to CSV
        writer.writerow([fname] + [detected_text[c] for c in expected_classes])

print(f"\n‚úÖ DONE! CSV saved to ‚Üí {output_csv_path}")
print(f"üóÇÔ∏è Individual TXT files in ‚Üí {output_txt_dir}")

import os
import cv2
import csv
import pytesseract
import re
import numpy as np
from PIL import Image
from ultralytics import YOLO

# ========== CONFIGURATION ==========
image_dir = "/content/drive/MyDrive/cheque_project/dataset"
output_csv_path = "/content/drive/MyDrive/cheque_project/cheque_data_combined.csv"
output_txt_dir = "/content/drive/MyDrive/cheque_project/combined_results_txts"
image_exts = (".jpg", ".jpeg", ".png")
expected_classes = ["bank name", "drawer name", "ifsc code", "account no"]

os.makedirs(output_txt_dir, exist_ok=True)

# ========== LOAD YOLOv8 MODEL ==========
model_path = "/content/drive/MyDrive/cheque_project/best.pt"
model = YOLO(model_path)

# ========== ENHANCED OCR PREPROCESSING ==========
def preprocess_for_ocr(img, min_height=30, is_account_no=False):
    # Convert to grayscale
    if len(img.shape) == 3:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    else:
        gray = img

    # Handle small images with upscaling
    h, w = gray.shape
    if h < min_height:
        scale_factor = min_height / h
        new_width = int(w * scale_factor)
        gray = cv2.resize(gray, (new_width, min_height), interpolation=cv2.INTER_CUBIC)
        h, w = gray.shape

    if h == 0 or w == 0:
        return gray

    # Special handling for account numbers
    if is_account_no:
        # Use different processing for digits
        denoised = cv2.fastNlMeansDenoising(gray, h=15, templateWindowSize=7, searchWindowSize=21)
        # Use Otsu's thresholding for better digit separation
        _, thresh = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        return thresh

    # Standard processing for text
    denoised = cv2.fastNlMeansDenoising(gray, h=10, templateWindowSize=7, searchWindowSize=21)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
    contrast_enhanced = clahe.apply(denoised)
    gaussian = cv2.GaussianBlur(contrast_enhanced, (0,0), 2.0)
    sharpened = cv2.addWeighted(contrast_enhanced, 1.5, gaussian, -0.5, 0)
    return cv2.adaptiveThreshold(
        sharpened, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY, 31, 10
    )

# ========== ENHANCED REGEX HELPERS ==========
def extract_ifsc(text):
    patterns = [
        r'\b[A-Z]{4}0[A-Z0-9]{6}\b',
        r'\b[A-Z]{4}0\d{6}\b',
        r'\b[A-Z]{4}0\w{6}\b'
    ]
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(0)
    return "Not Detected"

def extract_account_no(text, is_direct_ocr=False):
    """Enhanced account number extraction with multiple patterns"""
    # Pattern 1: Clean digit sequences (9-18 digits)
    clean_text = re.sub(r'\s+', '', text)  # Remove spaces
    match = re.search(r'\d{9,18}', clean_text)
    if match:
        return match.group(0)

    # Pattern 2: Account numbers with spaces (e.g., "1234 5678 9012")
    if not is_direct_ocr:
        spaced_match = re.search(r'(\d{4}\s\d{4}\s\d{4}(\s\d{4})?', text)
        if spaced_match:
            return re.sub(r'\s+', '', spaced_match.group(0))

    # Pattern 3: Account numbers with special characters (e.g., "1234-5678-9012")
    special_char_match = re.search(r'[\d\- ]{12,24}', text)
    if special_char_match:
        cleaned = re.sub(r'[^\d]', '', special_char_match.group(0))
        if 9 <= len(cleaned) <= 18:
            return cleaned

    return "Not Detected"

def extract_bank_name(text):
    lines = text.replace(':', ' ').replace('-', ' ').split('\n')
    for line in lines:
        clean = re.sub(r'[^A-Za-z\s]', '', line).strip()
        if "BANK" in clean.upper() and len(clean.split()) >= 2:
            return clean
    return "Not Detected"

def extract_name(text):
    lines = text.split('\n')
    exclusion_terms = ["ACCOUNT", "SAVINGS", "A/C", "BANK", "IFSC", "CODE", "CHEQUE", "BRANCH"]

    for line in lines:
        clean = re.sub(r'[^A-Za-z\s]', '', line).strip()
        if (clean.isupper() and 2 <= len(clean.split()) <= 4 and
            not any(term in clean.upper() for term in exclusion_terms)):
            return clean
    return "Not Detected"

# ========== DUAL OCR STRATEGY ==========
def run_dual_ocr_strategy(crop, label):
    """Run OCR with both ROI-focused and full-image approaches"""
    # First strategy: Process the cropped region
    is_account_no = (label == "account no")
    crop_proc = preprocess_for_ocr(crop, is_account_no=is_account_no)
    pil_img = Image.fromarray(crop_proc)

    # Special config for account numbers
    if is_account_no:
        ocr_configs = [
            '--oem 3 --psm 8 -c tessedit_char_whitelist=0123456789',  # Digits only
            '--oem 3 --psm 7 -c tessedit_char_whitelist=0123456789',  # Line with digits
            '--oem 3 --psm 7',
            '--oem 3 --psm 8'
        ]
    else:
        ocr_configs = [
            '--oem 3 --psm 7',  # Single text line
            '--oem 3 --psm 8',  # Single word
            '--oem 3 --psm 11'  # Sparse text
        ]

    best_text = ""
    for cfg in ocr_configs:
        text = pytesseract.image_to_string(pil_img, lang='eng', config=cfg).strip()
        # Validate text based on expected content
        if label == "ifsc code" and extract_ifsc(text) != "Not Detected":
            return text
        elif label == "account no" and extract_account_no(text) != "Not Detected":
            return text
        elif len(text) > len(best_text):
            best_text = text
    return best_text

# ========== MAIN PROCESSING ==========
with open(output_csv_path, "w", newline="", encoding="utf-8") as csv_file:
    writer = csv.writer(csv_file)
    writer.writerow(["image"] + [c.upper() for c in expected_classes])

    for fname in sorted(os.listdir(image_dir)):
        if not fname.lower().endswith(image_exts):
            continue

        img_path = os.path.join(image_dir, fname)
        image = cv2.imread(img_path)
        if image is None:
            print(f"‚ö†Ô∏è Skipping unreadable file: {fname}")
            continue

        detected_text = {label: "Not Detected" for label in expected_classes}
        h_img, w_img = image.shape[:2]

        # STRATEGY 1: YOLO-based ROI detection
        results = model.predict(image, conf=0.3, verbose=False)[0]
        sorted_boxes = sorted(results.boxes.data.tolist(), key=lambda box: (box[2]-box[0])*(box[3]-box[1]))

        for box in sorted_boxes:
            x1, y1, x2, y2, score, class_id = box
            label = model.names[int(class_id)].lower()

            if label not in expected_classes:
                continue

            pad = 8
            x1, y1 = max(0, int(x1)-pad), max(0, int(y1)-pad)
            x2, y2 = min(w_img, int(x2)+pad), min(h_img, int(y2)+pad)

            if x2 <= x1 or y2 <= y1:
                continue

            crop = image[y1:y2, x1:x2]
            text = run_dual_ocr_strategy(crop, label)

            if label == "ifsc code":
                detected_text[label] = extract_ifsc(text)
            elif label == "drawer name":
                detected_text[label] = extract_name(text)
            elif label == "account no":
                detected_text[label] = extract_account_no(text)
            elif label == "bank name":
                detected_text[label] = extract_bank_name(text)

            print(f"{fname} | {label:<15} ({score:.2f}) ‚Üí {detected_text[label]}")

        # STRATEGY 2: Full-image OCR fallback
        if any(v == "Not Detected" for v in detected_text.values()):
            full_proc = preprocess_for_ocr(image)
            full_pil = Image.fromarray(full_proc)
            full_text = pytesseract.image_to_string(
                full_pil,
                lang='eng',
                config='--oem 3 --psm 6'
            )

            # Special account number handling in full OCR
            if detected_text["account no"] == "Not Detected":
                detected_text["account no"] = extract_account_no(full_text, is_direct_ocr=True)

            # Standard extraction for other fields
            if detected_text["ifsc code"] == "Not Detected":
                detected_text["ifsc code"] = extract_ifsc(full_text)
            if detected_text["drawer name"] == "Not Detected":
                detected_text["drawer name"] = extract_name(full_text)
            if detected_text["bank name"] == "Not Detected":
                detected_text["bank name"] = extract_bank_name(full_text)

        # STRATEGY 3: Direct OCR for account numbers as last resort
        if detected_text["account no"] == "Not Detected":
            # Try with different preprocessing focused on digits
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            dilated = cv2.dilate(thresh, np.ones((3,3), np.uint8), iterations=1)
            account_pil = Image.fromarray(dilated)

            # Try multiple OCR configurations focused on digits
            digit_configs = [
                '--oem 3 --psm 11 -c tessedit_char_whitelist=0123456789',
                '--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789',
                '--oem 3 --psm 4'
            ]

            for cfg in digit_configs:
                digit_text = pytesseract.image_to_string(account_pil, lang='eng', config=cfg)
                acc_no = extract_account_no(digit_text, is_direct_ocr=True)
                if acc_no != "Not Detected":
                    detected_text["account no"] = acc_no
                    break

        # Write to text file
        txt_path = os.path.join(output_txt_dir, os.path.splitext(fname)[0] + ".txt")
        with open(txt_path, "w", encoding="utf-8") as f_txt:
            f_txt.write(f"FILE: {fname}\n")
            for label in expected_classes:
                label_upper = label.upper()
                f_txt.write(f"{label_upper}: {detected_text[label]}\n")

        # Write to CSV
        writer.writerow([fname] + [detected_text[c] for c in expected_classes])

print(f"\n‚úÖ DONE! CSV saved to ‚Üí {output_csv_path}")
print(f"üóÇÔ∏è Individual TXT files in ‚Üí {output_txt_dir}")